---
phase: 03-streaming-api-and-frontend
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/main.py
  - backend/checkpointer.py
  - backend/api/__init__.py
  - backend/api/chat.py
  - backend/api/corpus.py
  - backend/requirements.txt
autonomous: true
requirements:
  - UI-04
  - UI-06
  - UI-07

must_haves:
  truths:
    - "POST /api/chat/stream returns a text/event-stream response with token, stage, hitl_pause, and done events"
    - "POST /api/chat/resume resumes a paused HITL thread and returns a new SSE stream"
    - "POST /api/corpus/ingest accepts all four source types (text, file, gdrive, url) as multipart form data"
    - "GET /api/corpus/status returns current article count and chunk count"
    - "The FastAPI app starts without errors and the LangGraph graph is compiled once at startup via lifespan"
  artifacts:
    - path: "backend/main.py"
      provides: "FastAPI app with lifespan, CORS for localhost:3000, router mounts"
      contains: "lifespan"
    - path: "backend/checkpointer.py"
      provides: "AsyncSqliteSaver init helper used by lifespan"
      contains: "AsyncSqliteSaver"
    - path: "backend/api/chat.py"
      provides: "/api/chat/stream and /api/chat/resume endpoints"
      exports: ["router", "chat_stream", "chat_resume"]
    - path: "backend/api/corpus.py"
      provides: "/api/corpus/ingest and /api/corpus/status endpoints"
      exports: ["router", "ingest_corpus", "corpus_status"]
  key_links:
    - from: "backend/main.py"
      to: "backend/checkpointer.py"
      via: "lifespan context manager"
      pattern: "AsyncSqliteSaver"
    - from: "backend/api/chat.py"
      to: "app.state.graph"
      via: "astream_events version=v2"
      pattern: "astream_events"
    - from: "backend/api/chat.py"
      to: "interrupt detection"
      via: "stream_mode updates __interrupt__ key"
      pattern: "__interrupt__"
    - from: "backend/api/corpus.py"
      to: "Phase 1 ingestion handlers"
      via: "import from corpus module"
      pattern: "from.*corpus.*import"
---

<objective>
Create the FastAPI backend that exposes the LangGraph Author mode pipeline over HTTP with SSE streaming, HITL resume, and corpus management endpoints.

Purpose: This is the server-side bridge between the proven Phase 2 LangGraph backend and the Phase 3 Next.js frontend. All SSE streaming patterns, HITL pause/resume mechanics, and corpus ingestion routing live here.
Output: A running FastAPI server (`uvicorn backend.main:app --reload`) that the Next.js frontend can connect to directly for streaming, checkpoints, and corpus operations.
</objective>

<execution_context>
@/Users/franciszekmarzynski/.claude/get-shit-done/workflows/execute-plan.md
@/Users/franciszekmarzynski/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-streaming-api-and-frontend/03-RESEARCH.md
@.planning/phases/03-streaming-api-and-frontend/03-CONTEXT.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: FastAPI app scaffold with lifespan, CORS, and AsyncSqliteSaver</name>
  <files>
    backend/main.py
    backend/checkpointer.py
    backend/requirements.txt
    backend/api/__init__.py
  </files>
  <action>
Create `backend/requirements.txt` adding Phase 3 dependencies to whatever Phase 2 already has. Include:
```
fastapi>=0.115
uvicorn>=0.34
python-multipart>=0.0.12
```
(langgraph, langgraph-checkpoint-sqlite, and other Phase 2 deps already in requirements.txt — append only what is missing.)

Create `backend/checkpointer.py`:
```python
from contextlib import asynccontextmanager
from langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver

@asynccontextmanager
async def get_checkpointer(db_path: str = "checkpoints.db"):
    async with AsyncSqliteSaver.from_conn_string(db_path) as checkpointer:
        await checkpointer.setup()
        yield checkpointer
```

Create `backend/main.py`:
```python
from contextlib import asynccontextmanager
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from backend.checkpointer import get_checkpointer
from backend.api.chat import router as chat_router
from backend.api.corpus import router as corpus_router
from graph.graph import build_graph  # Phase 2 graph builder

@asynccontextmanager
async def lifespan(app: FastAPI):
    async with get_checkpointer("checkpoints.db") as checkpointer:
        app.state.graph = build_graph(checkpointer)
        yield

app = FastAPI(lifespan=lifespan, title="Bond Agent API")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

app.include_router(chat_router, prefix="/api/chat")
app.include_router(corpus_router, prefix="/api/corpus")

@app.get("/health")
async def health():
    return {"status": "ok"}
```

If `build_graph` does not yet exist (Phase 2 not executed), create a stub:
```python
# graph/graph.py (stub — replace when Phase 2 is complete)
def build_graph(checkpointer):
    from langgraph.graph import StateGraph, START, END
    builder = StateGraph(dict)
    builder.add_node("stub", lambda s: s)
    builder.add_edge(START, "stub")
    builder.add_edge("stub", END)
    return builder.compile(checkpointer=checkpointer)
```
Document the stub clearly with `# STUB — replace with Phase 2 graph`.

Create `backend/api/__init__.py` as empty file.
  </action>
  <verify>
    cd /Users/franciszekmarzynski/Downloads/Bond-agent && pip install -r backend/requirements.txt -q && python -c "from backend.main import app; print('import ok')"
  </verify>
  <done>FastAPI app imports without error; CORS middleware and lifespan are present in main.py; AsyncSqliteSaver is used in checkpointer.py</done>
</task>

<task type="auto">
  <name>Task 2: SSE chat stream and HITL resume endpoints</name>
  <files>backend/api/chat.py</files>
  <action>
Create `backend/api/chat.py` implementing two endpoints per the research patterns.

Use `astream_events` with `version="v2"` (not `astream` with `stream_mode` — this is the recommendation from the open question in RESEARCH.md). Filter `on_chat_model_stream` for tokens and `on_custom_event` for stage transitions. Use `stream_mode="updates"` check for `__interrupt__` detection.

**SSE Event schema** (canonical, from RESEARCH.md):
- `event: thread_id` — emitted first on new session
- `event: stage` — `{"stage": "research|structure|writing", "status": "running|complete"}`
- `event: token` — `{"token": "partial text"}` — only for writer node (filter by `langgraph_node == "write_draft"` or equivalent Phase 2 node name)
- `event: message` — `{"role": "assistant", "content": "full message"}` — for research result (appears all at once, NOT streamed token-by-token, per locked decision)
- `event: hitl_pause` — `{"checkpoint_id": "cp1|cp2", "type": "approve_reject", "iterations_remaining": 3}`
- `event: error` — `{"message": "...", "retryable": true}`
- `event: done` — `{}`

**CRITICAL anti-pattern to avoid (from research):** Place `interrupt()` at the START of checkpoint nodes in Phase 2 graph to avoid re-execution side effects. Document this assumption in a comment.

```python
import json, uuid
from fastapi import APIRouter, Request
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from typing import Optional
from langgraph.types import Command

router = APIRouter()

class ChatRequest(BaseModel):
    message: str
    thread_id: Optional[str] = None
    mode: str = "author"

class ResumeRequest(BaseModel):
    thread_id: str
    action: str   # "approve" | "approve_save" | "reject"
    feedback: Optional[str] = None

@router.post("/stream")
async def chat_stream(req: ChatRequest, request: Request):
    thread_id = req.thread_id or str(uuid.uuid4())
    config = {"configurable": {"thread_id": thread_id}}
    graph = request.app.state.graph

    async def generate():
        yield f"event: thread_id\ndata: {json.dumps({'thread_id': thread_id})}\n\n"
        try:
            async for event in graph.astream_events(
                {"messages": [{"role": "user", "content": req.message}], "mode": req.mode},
                config=config,
                version="v2",
            ):
                etype = event["event"]
                meta = event.get("metadata", {})

                if etype == "on_chat_model_stream":
                    node = meta.get("langgraph_node", "")
                    token = event["data"]["chunk"].content
                    if token:
                        # Only stream tokens for the draft writer node (not research)
                        # Research result is emitted as a single "message" event (locked UX decision)
                        if node in ("write_draft", "writer", "stylize_draft"):
                            yield f"event: token\ndata: {json.dumps({'token': token})}\n\n"

                elif etype == "on_custom_event" and event["name"] == "stage_change":
                    yield f"event: stage\ndata: {json.dumps(event['data'])}\n\n"

                elif etype == "on_custom_event" and event["name"] == "research_complete":
                    # Full research report — emit as single message (no token-by-token streaming)
                    yield f"event: message\ndata: {json.dumps({'role': 'assistant', 'content': event['data']['content']})}\n\n"

                elif etype == "on_custom_event" and event["name"] == "hitl_pause":
                    yield f"event: hitl_pause\ndata: {json.dumps(event['data'])}\n\n"
                    return  # Stream ends; client waits for user action

        except Exception as e:
            yield f"event: error\ndata: {json.dumps({'message': str(e), 'retryable': True})}\n\n"

        yield "event: done\ndata: {}\n\n"

    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",
            "Content-Encoding": "none",
        },
    )

@router.post("/resume")
async def chat_resume(req: ResumeRequest, request: Request):
    config = {"configurable": {"thread_id": req.thread_id}}
    graph = request.app.state.graph
    resume_value = {"action": req.action, "feedback": req.feedback}

    async def generate():
        try:
            async for event in graph.astream_events(
                Command(resume=resume_value),
                config=config,
                version="v2",
            ):
                etype = event["event"]
                meta = event.get("metadata", {})

                if etype == "on_chat_model_stream":
                    node = meta.get("langgraph_node", "")
                    token = event["data"]["chunk"].content
                    if token and node in ("write_draft", "writer", "stylize_draft"):
                        yield f"event: token\ndata: {json.dumps({'token': token})}\n\n"

                elif etype == "on_custom_event" and event["name"] == "stage_change":
                    yield f"event: stage\ndata: {json.dumps(event['data'])}\n\n"

                elif etype == "on_custom_event" and event["name"] == "hitl_pause":
                    yield f"event: hitl_pause\ndata: {json.dumps(event['data'])}\n\n"
                    return

        except Exception as e:
            yield f"event: error\ndata: {json.dumps({'message': str(e), 'retryable': True})}\n\n"

        yield "event: done\ndata: {}\n\n"

    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",
            "Content-Encoding": "none",
        },
    )
```
  </action>
  <verify>
    cd /Users/franciszekmarzynski/Downloads/Bond-agent && python -c "from backend.api.chat import router; print('chat router ok'); routes = [r.path for r in router.routes]; print(routes)"
  </verify>
  <done>chat.py imports cleanly; router has /stream and /resume routes; both return StreamingResponse with text/event-stream media type</done>
</task>

<task type="auto">
  <name>Task 3: Corpus ingest and status endpoints</name>
  <files>backend/api/corpus.py</files>
  <action>
Create `backend/api/corpus.py` implementing corpus management endpoints.

The ingest endpoint accepts all 4 source types via multipart form (requires python-multipart installed in Task 1). Route each source_type to the appropriate Phase 1 ingestion handler. If Phase 1 handlers do not yet exist (phase not executed), wrap the call in a try/except and return a `{"status": "stub", "message": "Phase 1 not yet executed"}` response — document clearly.

```python
from fastapi import APIRouter, UploadFile, File, Form, HTTPException
from typing import Optional

router = APIRouter()

@router.post("/ingest")
async def ingest_corpus(
    source_type: str = Form(...),           # "text" | "file" | "gdrive" | "url"
    text: Optional[str] = Form(None),
    file: Optional[UploadFile] = File(None),
    gdrive_folder_id: Optional[str] = Form(None),
    blog_url: Optional[str] = Form(None),
    source_label: str = Form("own"),         # "own" | "external"
):
    """
    Routes corpus ingestion to the appropriate Phase 1 handler.
    source_type must be one of: text, file, gdrive, url
    """
    try:
        if source_type == "text":
            if not text:
                raise HTTPException(status_code=422, detail="text field required for source_type=text")
            # Phase 1: ingest pasted text
            from corpus.ingest import ingest_text  # Phase 1 module
            result = await ingest_text(text=text, source_label=source_label)

        elif source_type == "file":
            if not file:
                raise HTTPException(status_code=422, detail="file field required for source_type=file")
            content = await file.read()
            from corpus.ingest import ingest_file
            result = await ingest_file(content=content, filename=file.filename, content_type=file.content_type, source_label=source_label)

        elif source_type == "gdrive":
            if not gdrive_folder_id:
                raise HTTPException(status_code=422, detail="gdrive_folder_id required for source_type=gdrive")
            from corpus.ingest import ingest_gdrive
            result = await ingest_gdrive(folder_id=gdrive_folder_id, source_label=source_label)

        elif source_type == "url":
            if not blog_url:
                raise HTTPException(status_code=422, detail="blog_url required for source_type=url")
            from corpus.ingest import ingest_url
            result = await ingest_url(url=blog_url, source_label=source_label)

        else:
            raise HTTPException(status_code=422, detail=f"Unknown source_type: {source_type}")

        return {"status": "ok", **result}

    except ImportError:
        # Phase 1 corpus module not yet available — stub response
        return {"status": "stub", "message": "Phase 1 corpus ingestion not yet implemented", "source_type": source_type}

@router.get("/status")
async def corpus_status():
    """Returns corpus article count, chunk count, and low-corpus warning flag."""
    try:
        from corpus.store import get_corpus_status  # Phase 1 module
        status = await get_corpus_status()
        return {
            "article_count": status["article_count"],
            "chunk_count": status["chunk_count"],
            "low_corpus_warning": status["article_count"] < 10,
        }
    except ImportError:
        # Phase 1 stub
        return {"article_count": 0, "chunk_count": 0, "low_corpus_warning": True, "status": "stub"}
```
  </action>
  <verify>
    cd /Users/franciszekmarzynski/Downloads/Bond-agent && python -c "from backend.api.corpus import router; print('corpus router ok'); routes = [r.path for r in router.routes]; print(routes)"
  </verify>
  <done>corpus.py imports cleanly; router has /ingest (POST) and /status (GET) routes; ingest handles all 4 source types with graceful ImportError stubs for Phase 1 modules</done>
</task>

</tasks>

<verification>
1. Run `cd /Users/franciszekmarzynski/Downloads/Bond-agent && uvicorn backend.main:app --port 8000 &` — server starts without errors
2. `curl http://localhost:8000/health` returns `{"status": "ok"}`
3. `curl http://localhost:8000/api/corpus/status` returns JSON with article_count, chunk_count, low_corpus_warning
4. `python -c "from backend.api.chat import router; from backend.api.corpus import router as cr; print('all imports ok')"` passes
5. Kill test server after verification
</verification>

<success_criteria>
- FastAPI server starts via `uvicorn backend.main:app --reload`
- `/health` returns 200
- `/api/chat/stream` POST returns `text/event-stream` content type
- `/api/chat/resume` POST accepts ResumeRequest and returns `text/event-stream`
- `/api/corpus/ingest` POST accepts multipart form with all 4 source_type values
- `/api/corpus/status` GET returns article_count and chunk_count
- All SSE responses include `X-Accel-Buffering: no` and `Cache-Control: no-cache` headers
- No synchronous blocking calls in stream generators
</success_criteria>

<output>
After completion, create `.planning/phases/03-streaming-api-and-frontend/03-01-SUMMARY.md`
</output>
