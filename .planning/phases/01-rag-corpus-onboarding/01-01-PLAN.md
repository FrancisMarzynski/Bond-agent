---
phase: 01-rag-corpus-onboarding
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - pyproject.toml
  - .env.example
  - bond/__init__.py
  - bond/config.py
  - bond/models.py
  - bond/store/__init__.py
  - bond/store/chroma.py
  - bond/store/article_log.py
  - bond/corpus/__init__.py
  - bond/corpus/chunker.py
  - bond/corpus/ingestor.py
  - bond/corpus/sources/__init__.py
  - bond/corpus/sources/text_source.py
  - bond/corpus/sources/file_source.py
  - bond/api/__init__.py
  - bond/api/main.py
  - bond/api/routes/__init__.py
  - bond/api/routes/corpus.py
autonomous: true
requirements:
  - CORP-01
  - CORP-02
  - CORP-05

must_haves:
  truths:
    - "Pasting text via POST /api/corpus/ingest/text with source_type stores chunks in ChromaDB and logs the article in SQLite"
    - "Uploading a PDF, DOCX, or TXT file via POST /api/corpus/ingest/file stores parsed chunks in ChromaDB"
    - "source_type='own' and source_type='external' are the only accepted values — any other value returns a validation error"
    - "Collection bond_style_corpus_v1 exists in ChromaDB with cosine similarity space and paraphrase-multilingual-MiniLM-L12-v2 embeddings"
  artifacts:
    - path: "bond/config.py"
      provides: "Pydantic Settings loading CHROMA_PATH, ARTICLE_DB_PATH, LOW_CORPUS_THRESHOLD, RAG_TOP_K, MAX_BLOG_POSTS, GOOGLE_CREDENTIALS_PATH env vars"
    - path: "bond/models.py"
      provides: "SourceType enum ('own'/'external'), IngestTextRequest, IngestFileResponse Pydantic models"
    - path: "bond/store/chroma.py"
      provides: "get_chroma_client() and get_or_create_corpus_collection() singletons"
    - path: "bond/store/article_log.py"
      provides: "SQLite article log with log_article(), get_article_count(), get_chunk_count() functions"
    - path: "bond/corpus/chunker.py"
      provides: "chunk_article() using RecursiveCharacterTextSplitter at 1875 chars / 190 overlap"
    - path: "bond/corpus/ingestor.py"
      provides: "CorpusIngestor.ingest(text, title, source_type, source_url) — adds chunks to ChromaDB, logs to SQLite"
    - path: "bond/corpus/sources/text_source.py"
      provides: "ingest_text(text, source_type, title) — wraps CorpusIngestor for plain text paste"
    - path: "bond/corpus/sources/file_source.py"
      provides: "extract_text(content, filename) — dispatches PDF/DOCX/TXT parsing; returns None and prints WARN on failure"
    - path: "bond/api/routes/corpus.py"
      provides: "POST /api/corpus/ingest/text and POST /api/corpus/ingest/file FastAPI routes"
    - path: "bond/api/main.py"
      provides: "FastAPI app with /api/corpus router mounted"
  key_links:
    - from: "bond/api/routes/corpus.py"
      to: "bond/corpus/ingestor.py"
      via: "CorpusIngestor.ingest() called in both endpoints"
      pattern: "CorpusIngestor"
    - from: "bond/corpus/ingestor.py"
      to: "bond/store/chroma.py"
      via: "collection.add() call"
      pattern: "collection\\.add"
    - from: "bond/corpus/ingestor.py"
      to: "bond/store/article_log.py"
      via: "log_article() call"
      pattern: "log_article"
    - from: "bond/corpus/sources/file_source.py"
      to: "pymupdf / python-docx"
      via: "extract_text_from_pdf / extract_text_from_docx"
      pattern: "import pymupdf|from docx import"
---

<objective>
Bootstrap the entire project structure and implement the two simplest ingestion paths: text paste (CORP-01) and file upload (CORP-02). This plan creates the shared foundation — config, models, ChromaDB store, SQLite article log, chunker, and CorpusIngestor — that Plans 02 and 03 depend on.

Purpose: Every subsequent plan in Phase 1 imports from bond/store/, bond/corpus/, and bond/models.py. Getting the foundation right here prevents refactoring debt in Plans 02 and 03.
Output: Runnable FastAPI application accepting text paste and file upload ingestion. The collection bond_style_corpus_v1 exists in ChromaDB after first startup.
</objective>

<execution_context>
@/Users/franciszekmarzynski/.claude/get-shit-done/workflows/execute-plan.md
@/Users/franciszekmarzynski/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-rag-corpus-onboarding/01-CONTEXT.md
@.planning/phases/01-rag-corpus-onboarding/01-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Project setup and shared foundation (config, models, store, chunker)</name>
  <files>
    pyproject.toml
    .env.example
    bond/__init__.py
    bond/config.py
    bond/models.py
    bond/store/__init__.py
    bond/store/chroma.py
    bond/store/article_log.py
    bond/corpus/__init__.py
    bond/corpus/chunker.py
    bond/corpus/sources/__init__.py
  </files>
  <action>
**1. pyproject.toml — uv-managed project**

Create pyproject.toml with project name "bond", python = ">=3.11". Add all Phase 1 dependencies:

```
dependencies = [
    "chromadb==1.5.1",
    "sentence-transformers>=3.0",
    "langchain-text-splitters",
    "trafilatura==2.0.0",
    "pymupdf",
    "python-docx",
    "google-api-python-client>=2.0",
    "google-auth>=2.0",
    "google-auth-oauthlib",
    "fastapi>=0.115",
    "uvicorn[standard]",
    "python-multipart",
    "pydantic>=2.0",
    "pydantic-settings",
    "python-dotenv",
    "langgraph-checkpoint-sqlite",
    "requests",
]

[dependency-groups]
dev = [
    "pytest",
    "pytest-asyncio",
    "ruff",
    "httpx",
]
```

Run: `uv sync`

**2. .env.example**

```
CHROMA_PATH=./data/chroma
ARTICLE_DB_PATH=./data/articles.db
LOW_CORPUS_THRESHOLD=10
RAG_TOP_K=5
MAX_BLOG_POSTS=50
# Google Drive: set GOOGLE_AUTH_METHOD=oauth or service_account
GOOGLE_AUTH_METHOD=oauth
GOOGLE_CREDENTIALS_PATH=./credentials.json
```

**3. bond/config.py — Pydantic Settings**

```python
from pydantic_settings import BaseSettings, SettingsConfigDict

class Settings(BaseSettings):
    model_config = SettingsConfigDict(env_file=".env", env_file_encoding="utf-8", extra="ignore")

    chroma_path: str = "./data/chroma"
    article_db_path: str = "./data/articles.db"
    low_corpus_threshold: int = 10
    rag_top_k: int = 5
    max_blog_posts: int = 50
    google_auth_method: str = "oauth"
    google_credentials_path: str = "./credentials.json"

settings = Settings()
```

**4. bond/models.py — SourceType enum and request/response models**

```python
from enum import Enum
from pydantic import BaseModel

class SourceType(str, Enum):
    OWN_TEXT = "own"
    EXTERNAL_BLOGGER = "external"

class IngestTextRequest(BaseModel):
    text: str
    title: str = "Untitled"
    source_type: SourceType

class IngestResult(BaseModel):
    article_id: str
    title: str
    chunks_added: int
    source_type: str
    warnings: list[str] = []
```

**5. bond/store/chroma.py — Singleton PersistentClient**

```python
import chromadb
from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction
from bond.config import settings

_client: chromadb.PersistentClient | None = None
_collection = None

def get_chroma_client() -> chromadb.PersistentClient:
    global _client
    if _client is None:
        _client = chromadb.PersistentClient(path=settings.chroma_path)
    return _client

def get_or_create_corpus_collection():
    global _collection
    if _collection is None:
        client = get_chroma_client()
        ef = SentenceTransformerEmbeddingFunction(
            model_name="paraphrase-multilingual-MiniLM-L12-v2",
            device="cpu",
        )
        _collection = client.get_or_create_collection(
            name="bond_style_corpus_v1",
            embedding_function=ef,
            metadata={"hnsw:space": "cosine"},
        )
    return _collection

def get_corpus_collection():
    """Get existing collection without creating it. Returns None if not initialized."""
    return _collection or get_or_create_corpus_collection()
```

NOTE: The embedding model (~420MB) will download on first call. This is expected behavior. The singleton pattern ensures the model loads only once per process lifetime.

**6. bond/store/article_log.py — SQLite article-level counter**

```python
import sqlite3
from datetime import datetime, timezone
from bond.config import settings

CREATE_TABLE = """
CREATE TABLE IF NOT EXISTS corpus_articles (
    article_id TEXT PRIMARY KEY,
    source_type TEXT NOT NULL,
    title TEXT,
    source_url TEXT DEFAULT '',
    chunk_count INTEGER DEFAULT 0,
    ingested_at TEXT
)
"""

def _get_conn() -> sqlite3.Connection:
    import os
    os.makedirs(os.path.dirname(os.path.abspath(settings.article_db_path)), exist_ok=True)
    conn = sqlite3.connect(settings.article_db_path)
    conn.execute(CREATE_TABLE)
    conn.commit()
    return conn

def log_article(article_id: str, source_type: str, title: str, source_url: str, chunk_count: int) -> None:
    conn = _get_conn()
    conn.execute(
        "INSERT OR REPLACE INTO corpus_articles (article_id, source_type, title, source_url, chunk_count, ingested_at) VALUES (?, ?, ?, ?, ?, ?)",
        (article_id, source_type, title, source_url, chunk_count, datetime.now(timezone.utc).isoformat()),
    )
    conn.commit()
    conn.close()

def get_article_count() -> int:
    conn = _get_conn()
    count = conn.execute("SELECT COUNT(*) FROM corpus_articles").fetchone()[0]
    conn.close()
    return count

def get_chunk_count() -> int:
    conn = _get_conn()
    count = conn.execute("SELECT SUM(chunk_count) FROM corpus_articles").fetchone()[0] or 0
    conn.close()
    return count
```

**7. bond/corpus/chunker.py — RecursiveCharacterTextSplitter**

```python
from langchain_text_splitters import RecursiveCharacterTextSplitter

# 1875 chars ≈ 500 tokens for Polish text (~3.75 chars/token average)
# 10% overlap for context continuity
_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1875,
    chunk_overlap=190,
    separators=["\n\n", "\n", " ", ""],
)

def chunk_article(text: str) -> list[str]:
    """Split article text into style-corpus chunks. Filters empty chunks."""
    chunks = _splitter.split_text(text)
    return [c for c in chunks if len(c.strip()) > 50]
```

All `__init__.py` files are empty (just `# bond package` comment).
  </action>
  <verify>
Run from project root:
```bash
uv sync
python -c "from bond.config import settings; print('config ok:', settings.chroma_path)"
python -c "from bond.models import SourceType, IngestTextRequest; r = IngestTextRequest(text='test', source_type=SourceType.OWN_TEXT); print('models ok:', r.source_type)"
python -c "from bond.corpus.chunker import chunk_article; chunks = chunk_article('Testowy artykuł. ' * 200); print(f'chunker ok: {len(chunks)} chunks')"
```
All three commands succeed with no ImportError.
  </verify>
  <done>uv sync installs all dependencies without error. All three python -c imports succeed and print confirmation strings.</done>
</task>

<task type="auto">
  <name>Task 2: CorpusIngestor, file_source, text_source, and FastAPI routes for paste + file upload</name>
  <files>
    bond/corpus/ingestor.py
    bond/corpus/sources/text_source.py
    bond/corpus/sources/file_source.py
    bond/api/__init__.py
    bond/api/main.py
    bond/api/routes/__init__.py
    bond/api/routes/corpus.py
  </files>
  <action>
**1. bond/corpus/ingestor.py — CorpusIngestor class**

```python
import uuid
from datetime import datetime, timezone
from bond.store.chroma import get_or_create_corpus_collection
from bond.store.article_log import log_article
from bond.corpus.chunker import chunk_article

class CorpusIngestor:
    def ingest(
        self,
        text: str,
        title: str,
        source_type: str,  # "own" | "external"
        source_url: str = "",
    ) -> dict:
        """
        Chunk text, embed into ChromaDB, log article to SQLite.
        Returns: {"article_id": str, "chunks_added": int}
        """
        chunks = chunk_article(text)
        if not chunks:
            return {"article_id": "", "chunks_added": 0}

        article_id = str(uuid.uuid4())
        now = datetime.now(timezone.utc).isoformat()

        collection = get_or_create_corpus_collection()
        ids = [f"{article_id}_{i}" for i in range(len(chunks))]
        metadatas = [
            {
                "source_type": source_type,
                "article_id": article_id,
                "article_title": title,
                "source_url": source_url,
                "ingested_at": now,
            }
            for _ in chunks
        ]
        collection.add(documents=chunks, metadatas=metadatas, ids=ids)
        log_article(article_id, source_type, title, source_url, len(chunks))

        return {"article_id": article_id, "chunks_added": len(chunks)}
```

**2. bond/corpus/sources/text_source.py — plain text paste**

```python
from bond.corpus.ingestor import CorpusIngestor

def ingest_text(text: str, source_type: str, title: str = "Pasted text") -> dict:
    ingestor = CorpusIngestor()
    return ingestor.ingest(text=text, title=title, source_type=source_type)
```

**3. bond/corpus/sources/file_source.py — PDF/DOCX/TXT extraction**

Use `import pymupdf` as canonical; add `fitz` fallback per research pitfall 2:

```python
import io

try:
    import pymupdf
    def _open_pdf(content: bytes):
        return pymupdf.open(stream=content, filetype="pdf")
except ImportError:
    import fitz as pymupdf
    def _open_pdf(content: bytes):
        return pymupdf.open(stream=content, filetype="pdf")

from docx import Document

ALLOWED_EXTENSIONS = {"pdf", "docx", "txt"}
MAX_FILE_SIZE = 20 * 1024 * 1024  # 20 MB

def extract_text_from_pdf(content: bytes) -> str | None:
    try:
        doc = _open_pdf(content)
        text = "\n\n".join(page.get_text() for page in doc)
        return text if text.strip() else None
    except Exception as e:
        print(f"WARN: PDF parse failed: {e} — skipping")
        return None

def extract_text_from_docx(content: bytes) -> str | None:
    try:
        doc = Document(io.BytesIO(content))
        paragraphs = [p.text for p in doc.paragraphs if p.text.strip()]
        return "\n\n".join(paragraphs) if paragraphs else None
    except Exception as e:
        print(f"WARN: DOCX parse failed: {e} — skipping")
        return None

def extract_text(content: bytes, filename: str) -> str | None:
    """Returns extracted text or None if extraction failed. Prints WARN on failure."""
    if len(content) > MAX_FILE_SIZE:
        print(f"WARN: {filename} exceeds 20MB limit — skipping")
        return None
    ext = filename.rsplit(".", 1)[-1].lower() if "." in filename else ""
    if ext not in ALLOWED_EXTENSIONS:
        print(f"WARN: Unsupported file type .{ext} in {filename} — skipping")
        return None
    if ext == "pdf":
        return extract_text_from_pdf(content)
    elif ext == "docx":
        return extract_text_from_docx(content)
    elif ext == "txt":
        try:
            return content.decode("utf-8", errors="replace")
        except Exception as e:
            print(f"WARN: TXT decode failed for {filename}: {e} — skipping")
            return None
    return None
```

**4. bond/api/routes/corpus.py — POST /api/corpus/ingest/text and /api/corpus/ingest/file**

```python
from fastapi import APIRouter, UploadFile, File, Form, HTTPException
from bond.models import IngestTextRequest, IngestResult, SourceType
from bond.corpus.sources.text_source import ingest_text
from bond.corpus.sources.file_source import extract_text
from bond.corpus.ingestor import CorpusIngestor

router = APIRouter(prefix="/api/corpus", tags=["corpus"])

@router.post("/ingest/text", response_model=IngestResult)
async def ingest_text_endpoint(request: IngestTextRequest):
    if not request.text.strip():
        raise HTTPException(status_code=422, detail="text must not be empty")
    result = ingest_text(
        text=request.text,
        source_type=request.source_type.value,
        title=request.title,
    )
    return IngestResult(
        article_id=result["article_id"],
        title=request.title,
        chunks_added=result["chunks_added"],
        source_type=request.source_type.value,
    )

@router.post("/ingest/file", response_model=IngestResult)
async def ingest_file_endpoint(
    file: UploadFile = File(...),
    source_type: str = Form(...),
    title: str = Form(default=""),
):
    # Validate source_type
    try:
        st = SourceType(source_type)
    except ValueError:
        raise HTTPException(status_code=422, detail=f"source_type must be 'own' or 'external', got: {source_type}")

    content = await file.read()
    filename = file.filename or "upload"
    effective_title = title or filename

    text = extract_text(content, filename)
    warnings = []
    if text is None:
        warnings.append(f"Could not parse {filename} — file skipped")
        return IngestResult(
            article_id="",
            title=effective_title,
            chunks_added=0,
            source_type=st.value,
            warnings=warnings,
        )

    ingestor = CorpusIngestor()
    result = ingestor.ingest(
        text=text,
        title=effective_title,
        source_type=st.value,
        source_url="",
    )
    return IngestResult(
        article_id=result["article_id"],
        title=effective_title,
        chunks_added=result["chunks_added"],
        source_type=st.value,
        warnings=warnings,
    )
```

**5. bond/api/main.py — FastAPI app entry point**

```python
from fastapi import FastAPI
from bond.api.routes.corpus import router as corpus_router

app = FastAPI(title="Bond — Agent Redakcyjny", version="0.1.0")
app.include_router(corpus_router)

@app.get("/health")
async def health():
    return {"status": "ok"}
```

To run locally: `uvicorn bond.api.main:app --reload`

All `__init__.py` files in bond/api/ and bond/api/routes/ are empty.
  </action>
  <verify>
```bash
# Start server (runs in background for testing)
uv run uvicorn bond.api.main:app --port 8001 &
sleep 5

# Health check
curl -s http://localhost:8001/health | python3 -c "import sys,json; d=json.load(sys.stdin); assert d['status']=='ok', d"

# Text ingest — own text
curl -s -X POST http://localhost:8001/api/corpus/ingest/text \
  -H "Content-Type: application/json" \
  -d '{"text": "To jest przykładowy artykuł testowy. Zawiera kilka zdań na temat stylometrii i analizy tekstu. Chcemy sprawdzić czy system poprawnie dzieli tekst na fragmenty i zapisuje metadane. Artykuł powinien mieć wystarczającą długość aby powstał przynajmniej jeden fragment. Dodajemy więcej treści aby upewnić się że chunker zadziała prawidłowo.", "title": "Test Article", "source_type": "own"}' \
  | python3 -c "import sys,json; d=json.load(sys.stdin); assert d['chunks_added'] > 0, d; print('text ingest ok:', d)"

# Invalid source_type — expect 422
curl -s -o /dev/null -w "%{http_code}" -X POST http://localhost:8001/api/corpus/ingest/text \
  -H "Content-Type: application/json" \
  -d '{"text": "test", "source_type": "invalid"}'

# Kill test server
kill %1 2>/dev/null || true
```

Expected: health returns `{"status":"ok"}`, text ingest returns `{"chunks_added": N > 0, ...}`, invalid source_type returns 422.
  </verify>
  <done>FastAPI server starts without errors. POST /api/corpus/ingest/text with source_type="own" returns 200 with chunks_added >= 1. POST with source_type="invalid" returns 422. The data/ directory is created with chroma/ and articles.db files.</done>
</task>

</tasks>

<verification>
After both tasks complete:
```bash
# Verify directory structure
ls bond/corpus/sources/   # text_source.py, file_source.py, __init__.py
ls bond/store/            # chroma.py, article_log.py, __init__.py
ls bond/api/routes/       # corpus.py, __init__.py

# Verify ChromaDB collection was created
python3 -c "
from bond.store.chroma import get_or_create_corpus_collection
col = get_or_create_corpus_collection()
print('Collection name:', col.name)
print('Chunk count:', col.count())
"

# Verify article log
python3 -c "
from bond.store.article_log import get_article_count, get_chunk_count
print('Articles:', get_article_count())
print('Chunks:', get_chunk_count())
"
```
</verification>

<success_criteria>
- uv sync completes with no errors
- bond_style_corpus_v1 ChromaDB collection exists with cosine similarity space
- POST /api/corpus/ingest/text stores chunks and returns chunks_added >= 1 for a multi-sentence input
- POST /api/corpus/ingest/file accepts PDF/DOCX/TXT; returns chunks_added >= 1 for valid file; returns warning for corrupt/unsupported file
- source_type enum validation rejects any value other than "own" or "external" with 422
- SQLite article_log records article count correctly (separate from chunk count)
</success_criteria>

<output>
After completion, create `.planning/phases/01-rag-corpus-onboarding/01-01-SUMMARY.md` documenting:
- Project structure created
- Key implementation decisions (singleton ChromaDB client, SQLite article log, chunk size rationale)
- Any deviations from planned approach
- Files created and their purposes
</output>
