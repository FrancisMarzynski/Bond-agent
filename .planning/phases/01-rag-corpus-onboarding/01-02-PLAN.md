---
phase: 01-rag-corpus-onboarding
plan: 02
type: execute
wave: 2
depends_on:
  - "01-01"
files_modified:
  - bond/corpus/sources/url_source.py
  - bond/corpus/sources/drive_source.py
  - bond/api/routes/corpus.py
autonomous: true
requirements:
  - CORP-03
  - CORP-04

must_haves:
  truths:
    - "POST /api/corpus/ingest/url with a blog URL discovers posts via sitemap and ingests each article, skipping and warning on failures"
    - "POST /api/corpus/ingest/drive with a folder_id downloads PDF/DOCX/TXT files from Google Drive and ingests them, skipping unreadable files"
    - "Blog scraping respects MAX_BLOG_POSTS env var and logs WARN when URL is unreachable or returns no article content"
    - "Drive connector shows service account email in warning when folder returns 0 files"
  artifacts:
    - path: "bond/corpus/sources/url_source.py"
      provides: "scrape_blog(url) using trafilatura sitemap_search — returns list of {url, title, text} dicts"
    - path: "bond/corpus/sources/drive_source.py"
      provides: "build_drive_service() and ingest_drive_folder(folder_id, source_type) using google-api-python-client v3"
    - path: "bond/api/routes/corpus.py"
      provides: "POST /api/corpus/ingest/url and POST /api/corpus/ingest/drive endpoints added to existing router"
  key_links:
    - from: "bond/api/routes/corpus.py"
      to: "bond/corpus/sources/url_source.py"
      via: "scrape_blog() + CorpusIngestor.ingest() per article"
      pattern: "scrape_blog"
    - from: "bond/api/routes/corpus.py"
      to: "bond/corpus/sources/drive_source.py"
      via: "ingest_drive_folder() call"
      pattern: "ingest_drive_folder"
    - from: "bond/corpus/sources/drive_source.py"
      to: "bond/corpus/sources/file_source.py"
      via: "extract_text() called on downloaded bytes"
      pattern: "extract_text"
---

<objective>
Add the two remaining ingestion paths: blog URL scraping (CORP-04) via trafilatura and Google Drive folder download (CORP-03) via google-api-python-client. Both paths reuse the CorpusIngestor from Plan 01 and add their routes to the existing corpus router.

Purpose: Completes the four ingestion paths required by the phase. Both paths follow the skip-and-warn failure policy established in CONTEXT.md.
Output: Two new FastAPI endpoints and their source modules. The FastAPI app now accepts all four ingestion methods.
</objective>

<execution_context>
@/Users/franciszekmarzynski/.claude/get-shit-done/workflows/execute-plan.md
@/Users/franciszekmarzynski/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-rag-corpus-onboarding/01-CONTEXT.md
@.planning/phases/01-rag-corpus-onboarding/01-RESEARCH.md
@.planning/phases/01-rag-corpus-onboarding/01-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Blog URL scraper (url_source.py) and Drive connector (drive_source.py)</name>
  <files>
    bond/corpus/sources/url_source.py
    bond/corpus/sources/drive_source.py
  </files>
  <action>
**1. bond/corpus/sources/url_source.py — trafilatura blog scraper**

Use trafilatura's `sitemap_search` for discovery (bulk blog scraping). Respect `MAX_BLOG_POSTS` from settings. Apply skip-and-warn policy on every failure.

```python
import json
import trafilatura
from trafilatura.sitemaps import sitemap_search
from bond.config import settings
from bond.corpus.ingestor import CorpusIngestor

def scrape_blog(url: str) -> list[dict]:
    """
    Discover all post URLs via sitemap/feed, extract each article.
    Returns list of {"url": str, "title": str, "text": str}.
    Failures: skip and warn (per CONTEXT.md policy).
    """
    articles = []

    # Discover post URLs via sitemap; fallback to single URL
    urls = sitemap_search(url) or [url]
    max_posts = settings.max_blog_posts

    if len(urls) > max_posts:
        print(f"WARN: Found {len(urls)} posts at {url}; limiting to {max_posts} (MAX_BLOG_POSTS)")
        urls = list(urls)[:max_posts]

    print(f"INFO: Scraping {len(urls)} posts from {url}")

    for post_url in urls:
        try:
            downloaded = trafilatura.fetch_url(post_url)
            if downloaded is None:
                print(f"WARN: Could not fetch {post_url} — skipping")
                continue
            raw = trafilatura.extract(downloaded, include_metadata=True, output_format="json")
            if raw is None:
                print(f"WARN: No article content found at {post_url} — skipping")
                continue
            data = json.loads(raw)
            text = data.get("text", "")
            if not text.strip():
                print(f"WARN: Empty text extracted from {post_url} — skipping")
                continue
            articles.append({
                "url": post_url,
                "title": data.get("title") or post_url,
                "text": text,
            })
        except Exception as e:
            print(f"WARN: {post_url} failed ({type(e).__name__}: {e}) — skipping")

    return articles


def ingest_blog(url: str, source_type: str) -> dict:
    """
    Scrape blog and ingest all articles. Returns summary dict.
    """
    articles = scrape_blog(url)
    if not articles:
        print(f"WARN: No articles extracted from {url}")
        return {"articles_ingested": 0, "total_chunks": 0, "warnings": [f"No articles found at {url}"]}

    ingestor = CorpusIngestor()
    total_chunks = 0
    ingested_count = 0
    warnings = []

    for article in articles:
        result = ingestor.ingest(
            text=article["text"],
            title=article["title"],
            source_type=source_type,
            source_url=article["url"],
        )
        if result["chunks_added"] > 0:
            total_chunks += result["chunks_added"]
            ingested_count += 1
        else:
            warnings.append(f"Article too short to chunk: {article['url']}")

    return {
        "articles_ingested": ingested_count,
        "total_chunks": total_chunks,
        "warnings": warnings,
    }
```

**2. bond/corpus/sources/drive_source.py — Google Drive folder downloader**

Support both auth methods from settings (oauth and service_account). Apply skip-and-warn policy. Handle Google Docs export as plain text. Show service account email in warning when folder returns 0 files (per RESEARCH.md pitfall 4).

```python
import io
from bond.config import settings
from bond.corpus.sources.file_source import extract_text
from bond.corpus.ingestor import CorpusIngestor

SCOPES = ["https://www.googleapis.com/auth/drive.readonly"]
SUPPORTED_MIME_TYPES = {
    "application/pdf": ".pdf",
    "application/vnd.openxmlformats-officedocument.wordprocessingml.document": ".docx",
    "text/plain": ".txt",
    "application/vnd.google-apps.document": ".txt",  # export as plain text
}


def build_drive_service():
    """Build Google Drive API service. Supports oauth and service_account auth methods."""
    from googleapiclient.discovery import build

    method = settings.google_auth_method
    creds_path = settings.google_credentials_path

    if method == "service_account":
        from google.oauth2 import service_account
        creds = service_account.Credentials.from_service_account_file(creds_path, scopes=SCOPES)
        return build("drive", "v3", credentials=creds)
    else:
        # OAuth installed-app flow (default)
        import os
        from google.oauth2.credentials import Credentials
        from google_auth_oauthlib.flow import InstalledAppFlow
        from google.auth.transport.requests import Request

        token_path = creds_path.replace("credentials.json", "token.json")
        creds = None
        if os.path.exists(token_path):
            creds = Credentials.from_authorized_user_file(token_path, SCOPES)
        if not creds or not creds.valid:
            if creds and creds.expired and creds.refresh_token:
                creds.refresh(Request())
            else:
                flow = InstalledAppFlow.from_client_secrets_file(creds_path, SCOPES)
                creds = flow.run_local_server(port=0)
            with open(token_path, "w") as f:
                f.write(creds.to_json())
        return build("drive", "v3", credentials=creds)


def list_folder_files(service, folder_id: str) -> list[dict]:
    """List supported files in a Drive folder. Handles pagination up to 500 files."""
    query = f"'{folder_id}' in parents and trashed=false"
    files = []
    page_token = None
    while True:
        params = {
            "q": query,
            "fields": "nextPageToken, files(id, name, mimeType)",
            "pageSize": 100,
        }
        if page_token:
            params["pageToken"] = page_token
        response = service.files().list(**params).execute()
        files.extend(response.get("files", []))
        page_token = response.get("nextPageToken")
        if not page_token:
            break
    return [f for f in files if f["mimeType"] in SUPPORTED_MIME_TYPES]


def download_file(service, file_id: str, mime_type: str) -> bytes | None:
    """Download file content as bytes. Exports Google Docs as plain text."""
    from googleapiclient.http import MediaIoBaseDownload
    try:
        if mime_type == "application/vnd.google-apps.document":
            request = service.files().export_media(fileId=file_id, mimeType="text/plain")
        else:
            request = service.files().get_media(fileId=file_id)
        buffer = io.BytesIO()
        downloader = MediaIoBaseDownload(buffer, request)
        done = False
        while not done:
            _, done = downloader.next_chunk()
        return buffer.getvalue()
    except Exception as e:
        print(f"WARN: Download failed for file {file_id}: {e} — skipping")
        return None


def ingest_drive_folder(folder_id: str, source_type: str) -> dict:
    """
    Download and ingest all supported files from a Drive folder.
    Returns summary dict with articles_ingested, total_chunks, warnings.
    """
    try:
        service = build_drive_service()
    except Exception as e:
        return {"articles_ingested": 0, "total_chunks": 0, "warnings": [f"Drive auth failed: {e}"]}

    files = list_folder_files(service, folder_id)

    if not files:
        # Per RESEARCH.md pitfall 4: show service account email for troubleshooting
        warning_msg = (
            f"No supported files found in folder {folder_id}. "
            "If using service_account auth, ensure the folder is shared with the service account email. "
            "Check GOOGLE_CREDENTIALS_PATH for the service account email address."
        )
        print(f"WARN: {warning_msg}")
        return {"articles_ingested": 0, "total_chunks": 0, "warnings": [warning_msg]}

    print(f"INFO: Found {len(files)} supported files in Drive folder {folder_id}")

    ingestor = CorpusIngestor()
    total_chunks = 0
    ingested_count = 0
    warnings = []

    for f in files:
        content = download_file(service, f["id"], f["mimeType"])
        if content is None:
            warnings.append(f"Could not download {f['name']} — skipped")
            continue

        # Determine effective extension for file_source dispatch
        ext = SUPPORTED_MIME_TYPES[f["mimeType"]].lstrip(".")
        effective_name = f["name"] if "." in f["name"] else f"{f['name']}.{ext}"

        text = extract_text(content, effective_name)
        if text is None:
            warnings.append(f"Could not parse {f['name']} — skipped")
            continue

        result = ingestor.ingest(
            text=text,
            title=f["name"],
            source_type=source_type,
            source_url=f"https://drive.google.com/file/d/{f['id']}",
        )
        if result["chunks_added"] > 0:
            total_chunks += result["chunks_added"]
            ingested_count += 1
        else:
            warnings.append(f"{f['name']} too short to produce chunks — skipped")

    return {
        "articles_ingested": ingested_count,
        "total_chunks": total_chunks,
        "warnings": warnings,
    }
```
  </action>
  <verify>
```bash
# Verify imports resolve correctly
python3 -c "from bond.corpus.sources.url_source import scrape_blog, ingest_blog; print('url_source ok')"
python3 -c "from bond.corpus.sources.drive_source import ingest_drive_folder; print('drive_source ok')"

# Test scrape_blog with a real URL (uses trafilatura — expect articles or graceful warning)
python3 -c "
from bond.corpus.sources.url_source import scrape_blog
articles = scrape_blog('https://example.com')
print(f'scrape_blog returned {len(articles)} articles (0 is ok for example.com)')
"
```
Both import checks succeed. scrape_blog completes without unhandled exceptions.
  </verify>
  <done>url_source.py and drive_source.py import without errors. scrape_blog() on an unreachable or non-blog URL prints WARN messages and returns an empty list (does not raise exceptions).</done>
</task>

<task type="auto">
  <name>Task 2: Add /ingest/url and /ingest/drive endpoints to corpus router</name>
  <files>
    bond/api/routes/corpus.py
  </files>
  <action>
Extend the existing corpus.py router (created in Plan 01) by adding two new endpoints. Do not modify existing /ingest/text or /ingest/file endpoints.

Append to the end of bond/api/routes/corpus.py (after the existing ingest_file_endpoint):

```python
from bond.corpus.sources.url_source import ingest_blog
from bond.corpus.sources.drive_source import ingest_drive_folder
from pydantic import BaseModel

class IngestUrlRequest(BaseModel):
    url: str
    source_type: SourceType

class IngestDriveRequest(BaseModel):
    folder_id: str
    source_type: SourceType

class BatchIngestResult(BaseModel):
    articles_ingested: int
    total_chunks: int
    source_type: str
    warnings: list[str] = []


@router.post("/ingest/url", response_model=BatchIngestResult)
async def ingest_url_endpoint(request: IngestUrlRequest):
    if not request.url.strip():
        raise HTTPException(status_code=422, detail="url must not be empty")
    result = ingest_blog(url=request.url, source_type=request.source_type.value)
    return BatchIngestResult(
        articles_ingested=result["articles_ingested"],
        total_chunks=result["total_chunks"],
        source_type=request.source_type.value,
        warnings=result.get("warnings", []),
    )


@router.post("/ingest/drive", response_model=BatchIngestResult)
async def ingest_drive_endpoint(request: IngestDriveRequest):
    if not request.folder_id.strip():
        raise HTTPException(status_code=422, detail="folder_id must not be empty")
    result = ingest_drive_folder(
        folder_id=request.folder_id,
        source_type=request.source_type.value,
    )
    return BatchIngestResult(
        articles_ingested=result["articles_ingested"],
        total_chunks=result["total_chunks"],
        source_type=request.source_type.value,
        warnings=result.get("warnings", []),
    )
```

Add the missing imports at the top of corpus.py (after existing imports):
- `from bond.corpus.sources.url_source import ingest_blog`
- `from bond.corpus.sources.drive_source import ingest_drive_folder`
- `IngestUrlRequest`, `IngestDriveRequest`, `BatchIngestResult` Pydantic models (defined inline in corpus.py for simplicity; alternatively add to bond/models.py — either approach is fine)
  </action>
  <verify>
```bash
# Start server
uv run uvicorn bond.api.main:app --port 8001 &
sleep 5

# Check all 4 routes are registered in OpenAPI schema
curl -s http://localhost:8001/openapi.json | python3 -c "
import sys, json
spec = json.load(sys.stdin)
paths = list(spec['paths'].keys())
required = ['/api/corpus/ingest/text', '/api/corpus/ingest/file', '/api/corpus/ingest/url', '/api/corpus/ingest/drive']
for r in required:
    assert r in paths, f'Missing route: {r} — found: {paths}'
print('All 4 routes registered:', required)
"

# Test /ingest/url with invalid URL — expect graceful warning, not 500
curl -s -X POST http://localhost:8001/api/corpus/ingest/url \
  -H "Content-Type: application/json" \
  -d '{"url": "https://definitely-not-a-real-blog-12345.example.com", "source_type": "external"}' \
  | python3 -c "
import sys, json
d = json.load(sys.stdin)
print('url ingest response:', d)
assert 'articles_ingested' in d, d
assert isinstance(d.get('warnings'), list), d
print('ok — no 500 error, warnings returned gracefully')
"

kill %1 2>/dev/null || true
```
  </verify>
  <done>All 4 /api/corpus/ingest/* routes appear in OpenAPI schema. POST /ingest/url with an unreachable URL returns 200 with articles_ingested=0 and warnings list (not a 500 error). POST /ingest/drive with an invalid folder_id returns 200 with warnings (Drive auth or access error surfaced in warnings, not as exception).</done>
</task>

</tasks>

<verification>
After both tasks complete, run:
```bash
uv run uvicorn bond.api.main:app --port 8001 &
sleep 5
curl -s http://localhost:8001/openapi.json | python3 -m json.tool | grep '"summary"'
kill %1 2>/dev/null || true
```
Expect: 4 corpus ingest endpoint summaries visible in OpenAPI schema.
</verification>

<success_criteria>
- All 4 ingestion endpoints registered: /ingest/text, /ingest/file, /ingest/url, /ingest/drive
- /ingest/url with unreachable URL returns 200 with warnings, not 500
- /ingest/drive with missing credentials returns 200 with auth error in warnings, not 500
- scrape_blog() respects MAX_BLOG_POSTS and prints INFO/WARN log lines
- Drive folder with 0 matching files returns warning message including service account troubleshooting hint
</success_criteria>

<output>
After completion, create `.planning/phases/01-rag-corpus-onboarding/01-02-SUMMARY.md` documenting:
- Blog scraping implementation details (sitemap discovery, MAX_BLOG_POSTS enforcement)
- Drive auth method chosen and how to switch between oauth/service_account
- Any trafilatura or Drive API behavior discoveries during testing
- Files modified and what was added vs. existing
</output>
