---
phase: 02-author-mode-backend
plan: 02
type: execute
wave: 2
depends_on:
  - 02-01
files_modified:
  - bond/graph/nodes/duplicate_check.py
  - bond/graph/nodes/researcher.py
  - bond/graph/graph.py
autonomous: true
requirements:
  - AUTH-02
  - AUTH-10
  - DUPL-01
  - DUPL-02
  - DUPL-03
  - DUPL-04
user_setup:
  - service: exa
    why: "Web research for Author mode pipeline"
    env_vars:
      - name: EXA_API_KEY
        source: "dashboard.exa.ai → API Keys"

must_haves:
  truths:
    - "When a topic closely matching an existing Metadata Log entry (similarity >= DUPLICATE_THRESHOLD) is submitted, the graph pauses with an interrupt containing existing_title, existing_date, and similarity_score"
    - "When no duplicate is found (or corpus is empty), duplicate_check_node returns duplicate_override=None and the graph proceeds to researcher"
    - "When the user resumes with override=False, the graph routes to END without calling researcher"
    - "researcher_node calls Exa search_and_contents once and stores title/url/summary (not full text) in search_cache; a second call with the same topic reads from cache without hitting Exa"
    - "research_report is a formatted Markdown string with a synthesis section followed by a numbered source list (Title, URL, 2-3 sentence summary per source)"
  artifacts:
    - path: "bond/graph/nodes/duplicate_check.py"
      provides: "duplicate_check_node function: embeds topic, queries ChromaDB metadata_log collection, interrupts if similarity >= DUPLICATE_THRESHOLD"
      exports: ["duplicate_check_node"]
    - path: "bond/graph/nodes/researcher.py"
      provides: "researcher_node function: session-cached Exa search, research report formatting, strips full text from cache after report generation"
      exports: ["researcher_node"]
  key_links:
    - from: "bond/graph/nodes/duplicate_check.py"
      to: "bond/store/chroma.py"
      via: "get_or_create_metadata_collection() for ChromaDB embedding query"
      pattern: "get_or_create_metadata_collection|metadata_log"
    - from: "bond/graph/nodes/duplicate_check.py"
      to: "langgraph.types.interrupt"
      via: "interrupt() call with duplicate warning payload"
      pattern: "interrupt\\("
    - from: "bond/graph/nodes/researcher.py"
      to: "exa_py.Exa"
      via: "Exa.search_and_contents() called only when topic not in search_cache"
      pattern: "search_and_contents"
    - from: "bond/graph/graph.py"
      to: "bond/graph/nodes/duplicate_check.py"
      via: "register_node('duplicate_check', duplicate_check_node)"
      pattern: "register_node.*duplicate_check"
---

<objective>
Implement the duplicate_check_node (DUPL-01 to DUPL-04) and researcher_node (AUTH-02, AUTH-10). These are the first two live nodes in the Author mode pipeline. After this plan, the graph can run from START through duplicate checking and web research before hitting a stub.

Purpose: Duplicate detection protects the user from repeating topics; the researcher node fetches and formats the web research report that Checkpoint 1 presents for approval. The session cache (AUTH-10) prevents double-billing Exa API calls.
Output: Two working LangGraph nodes registered in the graph. Duplicate detection with HITL interrupt. Exa-backed research report with session cache.
</objective>

<execution_context>
@/Users/franciszekmarzynski/.claude/get-shit-done/workflows/execute-plan.md
@/Users/franciszekmarzynski/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-author-mode-backend/02-CONTEXT.md
@.planning/phases/02-author-mode-backend/02-RESEARCH.md
@.planning/phases/02-author-mode-backend/02-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Duplicate check node (DUPL-01, DUPL-02, DUPL-03, DUPL-04)</name>
  <files>
    bond/graph/nodes/duplicate_check.py
    bond/store/chroma.py
    bond/graph/graph.py
  </files>
  <action>
**1. bond/store/chroma.py — add metadata_log collection helper**

Add a new function `get_or_create_metadata_collection()` to the existing bond/store/chroma.py. This creates/retrieves the `metadata_log` ChromaDB collection used for topic embedding queries. It uses the same SentenceTransformer embedding function as the corpus collection but is a separate collection named "bond_metadata_log_v1".

```python
# Add to bond/store/chroma.py (append after existing code)

_metadata_collection = None

def get_or_create_metadata_collection():
    """Get or create the metadata_log ChromaDB collection for duplicate topic detection."""
    global _metadata_collection
    if _metadata_collection is None:
        client = get_chroma_client()
        ef = SentenceTransformerEmbeddingFunction(
            model_name="paraphrase-multilingual-MiniLM-L12-v2",
            device="cpu",
        )
        _metadata_collection = client.get_or_create_collection(
            name="bond_metadata_log_v1",
            embedding_function=ef,
            metadata={"hnsw:space": "cosine"},
        )
    return _metadata_collection


def add_topic_to_metadata_collection(thread_id: str, topic: str, published_date: str) -> None:
    """Add a published topic embedding to the metadata_log collection.
    Called by save_metadata_node after article approval."""
    collection = get_or_create_metadata_collection()
    collection.add(
        ids=[thread_id],
        documents=[topic],
        metadatas=[{"title": topic, "published_date": published_date}],
    )
```

**2. bond/graph/nodes/duplicate_check.py**

Key rules to follow:
- ChromaDB returns cosine DISTANCE (0=identical, 2=opposite). Convert: `similarity = 1.0 - distance`
- interrupt() call must be the ONLY substantive logic in this function after the similarity check — no LLM calls, no Exa calls
- If the metadata_log collection is empty (no prior articles), query returns no results — handle gracefully and return duplicate_override=None
- DUPLICATE_THRESHOLD is loaded from settings (default 0.85 per user decision)

```python
import os
from langgraph.types import interrupt

from bond.config import settings
from bond.graph.state import AuthorState
from bond.store.chroma import get_or_create_metadata_collection


def duplicate_check_node(state: AuthorState) -> dict:
    """
    Check whether the incoming topic is too similar to a previously published article.

    Uses ChromaDB metadata_log collection with cosine similarity.
    Interrupts the graph if similarity >= DUPLICATE_THRESHOLD.
    User can override (proceed) or abort (route to END).
    """
    collection = get_or_create_metadata_collection()

    # Skip check if no prior articles in metadata log
    if collection.count() == 0:
        return {"duplicate_match": None, "duplicate_override": None}

    results = collection.query(
        query_texts=[state["topic"]],
        n_results=1,
        include=["metadatas", "distances"],
    )

    # results["ids"][0] is a list; empty if collection had 0 items (already handled above)
    if not results["ids"][0]:
        return {"duplicate_match": None, "duplicate_override": None}

    # ChromaDB returns DISTANCE; convert to similarity
    distance = results["distances"][0][0]
    similarity = 1.0 - distance

    if similarity < settings.duplicate_threshold:
        # Below threshold — no duplicate
        return {"duplicate_match": None, "duplicate_override": None}

    # Duplicate found — surface to user via interrupt
    match_meta = results["metadatas"][0][0]
    match_info = {
        "existing_title": match_meta.get("title", "Unknown"),
        "existing_date": match_meta.get("published_date", "Unknown"),
        "similarity_score": round(similarity, 3),
    }

    # interrupt() pauses the graph; resume value is bool (True=proceed, False=abort)
    proceed = interrupt({
        "warning": "Wykryto podobny temat",
        "existing_title": match_info["existing_title"],
        "existing_date": match_info["existing_date"],
        "similarity_score": match_info["similarity_score"],
    })

    return {
        "duplicate_match": match_info,
        "duplicate_override": bool(proceed),
    }
```

**3. bond/graph/graph.py — register duplicate_check_node**

Replace the stub `_duplicate_check_node` import and registration with the real implementation. At the top of graph.py, add:

```python
# Replace the stub definition with an import:
# (Remove the def _duplicate_check_node stub and replace with:)
from bond.graph.nodes.duplicate_check import duplicate_check_node as _duplicate_check_node
```

Update `_node_registry` to use the imported function:
```python
_node_registry: dict = {
    "duplicate_check": _duplicate_check_node,
    # other stubs remain
    ...
}
```
  </action>
  <verify>
```bash
cd /Users/franciszekmarzynski/Downloads/Bond-agent

# Verify import chain
python -c "
from bond.graph.nodes.duplicate_check import duplicate_check_node
from bond.store.chroma import get_or_create_metadata_collection
col = get_or_create_metadata_collection()
print('metadata_log collection ok, count:', col.count())
print('duplicate_check_node importable:', callable(duplicate_check_node))
"

# Verify graph still compiles with real node registered
python -c "
from bond.graph.graph import compile_graph
graph = compile_graph()
print('Graph compiles ok. Nodes:', sorted(graph.nodes.keys()))
"
```
Both commands complete without error. Collection is created (count >= 0).
  </verify>
  <done>duplicate_check_node is importable. ChromaDB bond_metadata_log_v1 collection exists (empty at this point). graph compile_graph() succeeds with the real duplicate_check_node registered.</done>
</task>

<task type="auto">
  <name>Task 2: Researcher node with Exa integration and session cache (AUTH-02, AUTH-10)</name>
  <files>
    bond/graph/nodes/researcher.py
    bond/graph/graph.py
  </files>
  <action>
**bond/graph/nodes/researcher.py**

Key rules:
- Check `search_cache` dict first; call Exa only on cache miss (AUTH-10)
- After generating the report, strip the `text` field from cached results — only keep title/url/summary to avoid state bloat (Pitfall 4 from research)
- Research report format (per locked user decision): synthesis section (2-3 paragraphs) FIRST, then numbered source list with Title / URL / 2-3 sentence summary per source
- RESEARCH_MODEL env var selects the LLM for report synthesis (AUTH-11)
- Number of Exa results: 8 (scales to topic complexity per Claude's discretion)
- Add exponential backoff for Exa rate limit errors (3 retries, backoff: 2s, 4s, 8s)

```python
import os
import time
from exa_py import Exa

from bond.config import settings
from bond.graph.state import AuthorState


def _call_exa_with_retry(exa: Exa, query: str, keywords: list[str], max_retries: int = 3) -> list[dict]:
    """Call Exa search_and_contents with exponential backoff on rate limit errors."""
    for attempt in range(max_retries):
        try:
            response = exa.search_and_contents(
                query=f"{query} {' '.join(keywords)}",
                num_results=8,
                type="auto",            # neural + keyword blend
                text={"max_characters": 2000},    # for synthesis; stripped from cache after
                summary={"query": query},         # abstractive summary per result
            )
            return [
                {
                    "title": r.title or "Brak tytułu",
                    "url": r.url or "",
                    "summary": r.summary or "",
                    "text": r.text or "",           # used for synthesis; stripped from cache after
                }
                for r in response.results
                if r.url  # filter results with no URL
            ]
        except Exception as e:
            err_str = str(e).lower()
            if "rate" in err_str or "429" in err_str:
                wait = 2 ** attempt
                print(f"Exa rate limit hit, waiting {wait}s (attempt {attempt + 1}/{max_retries})")
                time.sleep(wait)
                continue
            raise  # non-rate-limit errors propagate immediately
    raise RuntimeError(f"Exa API rate limit exceeded after {max_retries} retries")


def _format_research_report(results: list[dict], topic: str, keywords: list[str]) -> str:
    """
    Format results into Markdown report.

    Structure (per locked user decision):
    1. Synthesis section: 2-3 paragraphs summarizing key themes across sources
    2. Numbered source list: Title / URL / 2-3 sentence summary per source
    """
    from langchain_anthropic import ChatAnthropic
    from langchain_openai import ChatOpenAI

    research_model = settings.research_model

    # Build context for synthesis from article texts
    source_texts = "\n\n".join(
        f"### {r['title']}\nURL: {r['url']}\n{r['text'][:1000]}"
        for r in results
        if r.get("text")
    )

    synthesis_prompt = f"""Jesteś redaktorem. Na podstawie poniższych artykułów napisz krótką syntezę (2-3 akapity)
głównych tematów i trendów dotyczących: "{topic}" (słowa kluczowe: {', '.join(keywords)}).
Pisz po polsku. Nie cytuj źródeł bezpośrednio — syntezuj idee.

ARTYKUŁY:
{source_texts}

SYNTEZA:"""

    # Select LLM based on RESEARCH_MODEL env var
    if "claude" in research_model.lower() or "anthropic" in research_model.lower():
        llm = ChatAnthropic(model=research_model, max_tokens=600)
    else:
        llm = ChatOpenAI(model=research_model, max_tokens=600)

    synthesis = llm.invoke(synthesis_prompt).content.strip()

    # Build numbered source list
    source_lines = []
    for i, r in enumerate(results, 1):
        summary = r.get("summary") or r.get("text", "")[:300]
        source_lines.append(f"{i}. **{r['title']}**  \n   {r['url']}  \n   {summary}\n")

    sources_section = "\n".join(source_lines)

    return f"""## Raport z badań: {topic}

### Synteza

{synthesis}

---

### Źródła

{sources_section}"""


def researcher_node(state: AuthorState) -> dict:
    """
    Perform web research via Exa. Checks session cache first (AUTH-10).

    Returns updated search_cache (with text stripped to save state space)
    and formatted research_report (Markdown).
    """
    topic = state["topic"]
    keywords = state.get("keywords", [])
    cache = state.get("search_cache", {})

    if topic in cache:
        # Cache hit — no Exa API call
        raw_results = cache[topic]
    else:
        # Cache miss — call Exa
        exa = Exa(api_key=settings.exa_api_key)
        raw_results = _call_exa_with_retry(exa, topic, keywords)

    # Format report (uses text field for synthesis)
    report = _format_research_report(raw_results, topic, keywords)

    # Strip text from cache after report generation to avoid state bloat (Pitfall 4)
    slim_results = [
        {"title": r["title"], "url": r["url"], "summary": r.get("summary", "")}
        for r in raw_results
    ]
    cache[topic] = slim_results

    return {
        "research_report": report,
        "search_cache": cache,
    }
```

**bond/graph/graph.py — register researcher_node**

Replace the stub `_researcher_node` definition with an import at the top of graph.py:

```python
# Replace _researcher_node stub:
from bond.graph.nodes.researcher import researcher_node as _researcher_node
```

Update `_node_registry["researcher"]` to use the imported function.
  </action>
  <verify>
```bash
cd /Users/franciszekmarzynski/Downloads/Bond-agent

# Verify import chain (does NOT call Exa — just checks imports)
python -c "
from bond.graph.nodes.researcher import researcher_node
print('researcher_node importable:', callable(researcher_node))
"

# Verify graph still compiles with both real nodes
python -c "
from bond.graph.graph import compile_graph
graph = compile_graph()
print('Graph compiles ok with 2 real nodes.')
"
```

Note: A live Exa API integration test requires EXA_API_KEY in .env. If EXA_API_KEY is not set, set it first:
```bash
# Copy .env.example to .env and fill in EXA_API_KEY, then:
python -c "
import os
from dotenv import load_dotenv
load_dotenv()
from bond.graph.nodes.researcher import _call_exa_with_retry
from exa_py import Exa
from bond.config import settings
if settings.exa_api_key:
    exa = Exa(api_key=settings.exa_api_key)
    results = _call_exa_with_retry(exa, 'content marketing dla B2B', ['SEO', 'blog'])
    assert len(results) > 0, 'No results returned'
    assert results[0]['title'], 'No title in result'
    assert 'text' not in results[0] or results[0].get('text'), 'text field check'
    print(f'Exa live test ok: {len(results)} results, first title: {results[0][\"title\"]}')
else:
    print('EXA_API_KEY not set — skipping live test (set in .env to test)')
"
```
  </verify>
  <done>researcher_node imports without error. graph compile_graph() succeeds. When EXA_API_KEY is set: live Exa call returns >= 1 result with title and url. research_report returned by researcher_node starts with "## Raport z badań:".</done>
</task>

</tasks>

<verification>
After both tasks complete:
```bash
cd /Users/franciszekmarzynski/Downloads/Bond-agent

# Full import chain for both new nodes
python -c "
from bond.graph.nodes.duplicate_check import duplicate_check_node
from bond.graph.nodes.researcher import researcher_node
from bond.graph.graph import compile_graph
graph = compile_graph()
print('Both nodes imported and graph compiled.')
print('Nodes in graph:', sorted(graph.nodes.keys()))
"

# Verify metadata collection exists and ChromaDB path is correct
python -c "
from bond.store.chroma import get_or_create_metadata_collection
col = get_or_create_metadata_collection()
assert col.name == 'bond_metadata_log_v1', col.name
print('Metadata collection name:', col.name, '— ok')
"
```
</verification>

<success_criteria>
- duplicate_check_node imports without error; handles empty collection gracefully (returns duplicate_override=None)
- ChromaDB bond_metadata_log_v1 collection is created with cosine similarity space
- researcher_node imports without error
- search_cache stores only title/url/summary (not full text) after first call
- research_report output starts with "## Raport z badań:" and contains "### Synteza" and "### Źródła" sections
- compile_graph() succeeds with both nodes registered (not stubs)
</success_criteria>

<output>
After completion, create `.planning/phases/02-author-mode-backend/02-02-SUMMARY.md` documenting:
- Duplicate check node logic (ChromaDB query, distance-to-similarity conversion, interrupt payload)
- Researcher node design (Exa call, cache strategy, report format)
- Exa API rate limit handling
- Any deviations from planned approach
</output>
