---
phase: 02-author-mode-backend
plan: 04
type: execute
wave: 4
depends_on:
  - 02-03
files_modified:
  - bond/graph/nodes/checkpoint_2.py
  - bond/graph/nodes/save_metadata.py
  - bond/graph/graph.py
  - bond/harness.py
autonomous: false
requirements:
  - AUTH-07
  - AUTH-08
  - AUTH-09

must_haves:
  truths:
    - "checkpoint_2_node pauses via interrupt() exposing draft, draft_validated status, and cp2_iterations; on rejection the user specifies which sections to revise and the soft cap warning appears after 3 iterations (no hard block)"
    - "save_metadata_node writes topic, date, mode, and thread_id to bond_metadata.db metadata_log table AND adds the topic embedding to ChromaDB bond_metadata_log_v1 collection (for future DUPL-01 checks)"
    - "The test harness (harness.py) can drive the complete Author mode pipeline from topic input through both HITL checkpoints to final metadata save, simulating approve and reject-then-approve flows"
    - "Running the harness with approve-all results in: bond_metadata.db containing one metadata_log row, bond_metadata_log_v1 ChromaDB collection containing one embedding"
  artifacts:
    - path: "bond/graph/nodes/checkpoint_2.py"
      provides: "checkpoint_2_node: single interrupt() exposing draft + draft_validated + cp2_iterations; soft-cap warning at iterations >= 3; processes targeted section feedback"
      exports: ["checkpoint_2_node"]
    - path: "bond/graph/nodes/save_metadata.py"
      provides: "save_metadata_node: saves to SQLite metadata_log AND ChromaDB bond_metadata_log_v1; returns metadata_saved=True"
      exports: ["save_metadata_node"]
    - path: "bond/harness.py"
      provides: "CLI test harness: drives graph through both HITL checkpoints with configurable approve/reject decisions"
      exports: ["run_author_pipeline"]
  key_links:
    - from: "bond/graph/nodes/save_metadata.py"
      to: "bond/db/metadata_log.py"
      via: "save_article_metadata() call"
      pattern: "save_article_metadata"
    - from: "bond/graph/nodes/save_metadata.py"
      to: "bond/store/chroma.py"
      via: "add_topic_to_metadata_collection() to enable future duplicate detection"
      pattern: "add_topic_to_metadata_collection"
    - from: "bond/harness.py"
      to: "bond/graph/graph.py"
      via: "compile_graph() then invoke() / Command(resume=...)"
      pattern: "compile_graph|Command"
---

<objective>
Complete the Author mode pipeline by implementing Checkpoint 2, metadata save, and the CLI test harness. After this plan, the full pipeline is functional end-to-end in Python: from topic submission through both HITL checkpoints to metadata persistence. A human verification checkpoint confirms the complete flow works.

Purpose: The test harness is the primary integration test for Phase 2 — it proves the pipeline works end-to-end before Phase 3 adds a frontend. The metadata save node closes the duplicate detection loop: topics saved here become the corpus for future DUPL-01 checks.
Output: Complete Author mode Python backend. Two remaining stub nodes replaced. CLI harness drives the full graph. Human confirms the flow produces a draft and saves metadata.
</objective>

<execution_context>
@/Users/franciszekmarzynski/.claude/get-shit-done/workflows/execute-plan.md
@/Users/franciszekmarzynski/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-author-mode-backend/02-CONTEXT.md
@.planning/phases/02-author-mode-backend/02-RESEARCH.md
@.planning/phases/02-author-mode-backend/02-01-SUMMARY.md
@.planning/phases/02-author-mode-backend/02-03-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Checkpoint 2 node and metadata save node (AUTH-07, AUTH-08, AUTH-09)</name>
  <files>
    bond/graph/nodes/checkpoint_2.py
    bond/graph/nodes/save_metadata.py
    bond/graph/graph.py
  </files>
  <action>
**1. bond/graph/nodes/checkpoint_2.py**

Per locked user decisions:
- Rejection model: targeted revision — user specifies which sections to redo; others remain unchanged
- The 3-iteration limit is a SOFT CAP: after 3 iterations, continue with a warning (no hard block)
- Session recovery: SqliteSaver handles this automatically — the interrupted state is persisted

CRITICAL anti-pattern: do NOT put any LLM or Exa calls in this node. Only the interrupt() call.

```python
from langgraph.types import interrupt

from bond.graph.state import AuthorState

SOFT_CAP_ITERATIONS = 3


def checkpoint_2_node(state: AuthorState) -> dict:
    """
    Checkpoint 2: pause for human review of the stylized draft.

    Surfaces: draft, draft_validated, cp2_iterations.
    Resume format:
      Approve: {"approved": True}
      Reject:  {"approved": False, "feedback": "Sekcja 'Jak zacząć' jest zbyt ogólna..."}

    Soft cap: after SOFT_CAP_ITERATIONS rejections, continue with a warning (no hard block).
    Targeted revision: cp2_feedback passed to writer_node to revise flagged sections only.
    """
    cp2_iterations = state.get("cp2_iterations", 0)
    draft_validated = state.get("draft_validated", True)

    # Build interrupt payload
    interrupt_payload = {
        "checkpoint": "checkpoint_2",
        "draft": state.get("draft", ""),
        "draft_validated": draft_validated,
        "cp2_iterations": cp2_iterations,
        "instructions": (
            "Zatwierdź lub odrzuć draft. Przy odrzuceniu: wskaż konkretne sekcje do poprawki — "
            "pozostałe sekcje zostaną zachowane bez zmian."
        ),
    }

    # Soft cap warning (does NOT block — user can still reject/approve)
    if cp2_iterations >= SOFT_CAP_ITERATIONS:
        interrupt_payload["warning"] = (
            f"Uwaga: przekroczono {SOFT_CAP_ITERATIONS} iteracje poprawek. "
            "Możesz kontynuować lub zatwierdzić obecną wersję."
        )

    # Add draft_validated failure details if present
    if not draft_validated:
        interrupt_payload["validation_warning"] = (
            "Draft nie spełnia wszystkich wymogów SEO po automatycznych poprawkach. "
            "Rozważ zatwierdzenie i ręczną edycję lub odrzuć z feedbackiem."
        )

    user_response = interrupt(interrupt_payload)

    if user_response.get("approved"):
        return {"cp2_approved": True}

    # Rejection: capture targeted feedback for writer_node
    feedback = user_response.get("feedback", "")
    return {
        "cp2_approved": False,
        "cp2_feedback": feedback,
        "cp2_iterations": cp2_iterations + 1,
    }
```

**2. bond/graph/nodes/save_metadata.py**

Saves to two places (per AUTH-09 + DUPL-01 loop):
1. SQLite metadata_log table (bond_metadata.db) — relational record of published article
2. ChromaDB bond_metadata_log_v1 collection — topic embedding for future duplicate detection

```python
from datetime import datetime, timezone

from bond.db.metadata_log import save_article_metadata
from bond.graph.state import AuthorState
from bond.store.chroma import add_topic_to_metadata_collection


def save_metadata_node(state: AuthorState) -> dict:
    """
    Save published article metadata after Checkpoint 2 approval.

    Writes to:
    - SQLite metadata_log (bond_metadata.db) — relational record
    - ChromaDB bond_metadata_log_v1 — topic embedding for DUPL-01 future checks
    """
    topic = state["topic"]
    thread_id = state["thread_id"]
    published_date = datetime.now(timezone.utc).isoformat()

    # 1. Relational record in SQLite metadata_log
    save_article_metadata(
        thread_id=thread_id,
        topic=topic,
        mode="author",
    )

    # 2. Topic embedding in ChromaDB for future duplicate detection
    add_topic_to_metadata_collection(
        thread_id=thread_id,
        topic=topic,
        published_date=published_date,
    )

    print(f"Metadata saved: topic='{topic}', thread_id={thread_id}")
    return {"metadata_saved": True}
```

**3. bond/graph/graph.py — register both final nodes**

```python
# Add to imports in graph.py:
from bond.graph.nodes.checkpoint_2 import checkpoint_2_node as _checkpoint_2_node
from bond.graph.nodes.save_metadata import save_metadata_node as _save_metadata_node

# Update _node_registry:
_node_registry["checkpoint_2"] = _checkpoint_2_node
_node_registry["save_metadata"] = _save_metadata_node
```

All 7 nodes are now real implementations. The stub functions can be removed from graph.py, or left as dead code (they are no longer referenced by _node_registry).
  </action>
  <verify>
```bash
cd /Users/franciszekmarzynski/Downloads/Bond-agent

python -c "
from bond.graph.nodes.checkpoint_2 import checkpoint_2_node
from bond.graph.nodes.save_metadata import save_metadata_node
print('checkpoint_2_node importable:', callable(checkpoint_2_node))
print('save_metadata_node importable:', callable(save_metadata_node))
"

python -c "
from bond.graph.graph import compile_graph
graph = compile_graph()
print('All 7 nodes registered, graph compiles:', sorted(graph.nodes.keys()))
"

# Test save_metadata writes to both SQLite and ChromaDB
python -c "
import uuid
from bond.graph.nodes.save_metadata import save_metadata_node

test_state = {
    'topic': 'Test topic dla weryfikacji zapisu metadanych',
    'thread_id': str(uuid.uuid4()),
    'keywords': ['test'],
    'search_cache': {},
    'cp1_iterations': 0,
    'cp2_iterations': 1,
    'metadata_saved': False,
    'duplicate_match': None,
    'duplicate_override': None,
    'research_report': 'Test report',
    'heading_structure': '# Test\n## Sub',
    'cp1_approved': True,
    'cp1_feedback': None,
    'draft': 'Test draft',
    'draft_validated': True,
    'cp2_approved': True,
    'cp2_feedback': None,
}
result = save_metadata_node(test_state)
assert result['metadata_saved'] == True, result

from bond.db.metadata_log import get_recent_articles
rows = get_recent_articles(limit=5)
assert any(r['topic'] == test_state['topic'] for r in rows), f'Row not found: {rows}'

from bond.store.chroma import get_or_create_metadata_collection
col = get_or_create_metadata_collection()
assert col.count() >= 1, f'Embedding not saved, count: {col.count()}'

print('save_metadata_node ok: SQLite row saved, ChromaDB count:', col.count())
"
```
  </verify>
  <done>checkpoint_2_node and save_metadata_node import without error. compile_graph() succeeds with all 7 real nodes. save_metadata_node creates a row in SQLite metadata_log AND adds an embedding to ChromaDB bond_metadata_log_v1.</done>
</task>

<task type="auto">
  <name>Task 2: CLI test harness for end-to-end pipeline validation (AUTH-01)</name>
  <files>
    bond/harness.py
  </files>
  <action>
**bond/harness.py — CLI test harness**

The harness drives the graph through both HITL checkpoints programmatically. It supports two decision modes:
- `approve_all=True` (default for smoke testing): approve at every checkpoint
- Interactive mode: print checkpoint payload and wait for user input

The harness also validates:
- Session resume: if a thread_id already exists in bond_checkpoints.db, it resumes from the last checkpoint
- Thread ID collision: generates a new UUID for each fresh run to avoid Pitfall 5

```python
"""
bond/harness.py — CLI test harness for Author mode pipeline

Usage:
    # Approve-all smoke test (new session)
    python -m bond.harness

    # Interactive mode (prompts at each checkpoint)
    python -m bond.harness --interactive

    # Resume interrupted session
    python -m bond.harness --resume --thread-id <uuid>

    # Custom topic
    python -m bond.harness --topic "Marketing treści dla SaaS" --keywords "content marketing,SEO,blog"
"""

import argparse
import json
import uuid
from typing import Optional

from dotenv import load_dotenv

load_dotenv()  # Must be before bond.graph imports so env vars are available

from langgraph.types import Command

from bond.graph.graph import compile_graph


def _handle_interrupt(result: dict, interactive: bool) -> dict:
    """
    Extract interrupt payload from graph result and determine resume value.

    Returns the value to pass to Command(resume=...).
    """
    interrupts = result.get("__interrupt__", [])
    if not interrupts:
        return {}

    interrupt_data = interrupts[0].value if hasattr(interrupts[0], "value") else interrupts[0]
    checkpoint = interrupt_data.get("checkpoint", "unknown")

    print(f"\n{'='*60}")
    print(f"CHECKPOINT: {checkpoint.upper()}")
    print(f"{'='*60}")

    if checkpoint == "checkpoint_1":
        print("\n[RAPORT Z BADAŃ]")
        print(interrupt_data.get("research_report", "")[:500] + "...")
        print("\n[PROPONOWANA STRUKTURA NAGŁÓWKÓW]")
        print(interrupt_data.get("heading_structure", ""))
        print(f"\nIteracja: {interrupt_data.get('cp1_iterations', 0)}")

        if not interactive:
            print("\n[AUTO] Zatwierdzam strukturę nagłówków.")
            return {"approved": True}

        user_input = input("\nZatwierdź? [t/n]: ").strip().lower()
        if user_input == "t":
            return {"approved": True}
        edited = input("Wklej edytowaną strukturę nagłówków (lub Enter aby zachować obecną):\n")
        note = input("Opcjonalna notatka dla agenta:\n")
        return {
            "approved": False,
            "edited_structure": edited or interrupt_data.get("heading_structure", ""),
            "note": note,
        }

    elif checkpoint == "checkpoint_2":
        print("\n[DRAFT ARTYKUŁU]")
        print(interrupt_data.get("draft", "")[:800] + "\n[... draft skrócony do 800 znaków ...]")
        print(f"\nWalidacja SEO: {interrupt_data.get('draft_validated', 'n/a')}")
        print(f"Iteracja: {interrupt_data.get('cp2_iterations', 0)}")
        if interrupt_data.get("warning"):
            print(f"OSTRZEŻENIE: {interrupt_data['warning']}")

        if not interactive:
            print("\n[AUTO] Zatwierdzam draft.")
            return {"approved": True}

        user_input = input("\nZatwierdź? [t/n]: ").strip().lower()
        if user_input == "t":
            return {"approved": True}
        feedback = input("Opisz które sekcje poprawić i jak:\n")
        return {"approved": False, "feedback": feedback}

    elif "warning" in interrupt_data:
        # Duplicate detection checkpoint
        print(f"\n[DUPLIKAT] {interrupt_data.get('warning')}")
        print(f"Istniejący artykuł: {interrupt_data.get('existing_title')}")
        print(f"Data publikacji: {interrupt_data.get('existing_date')}")
        print(f"Podobieństwo: {interrupt_data.get('similarity_score')}")

        if not interactive:
            print("\n[AUTO] Kontynuuję (override=True).")
            return True  # proceed

        user_input = input("\nKontynuować pomimo podobieństwa? [t/n]: ").strip().lower()
        return user_input == "t"

    return {"approved": True}  # fallback


def run_author_pipeline(
    topic: str = "Jak zwiększyć ruch na blogu firmowym",
    keywords: Optional[list[str]] = None,
    thread_id: Optional[str] = None,
    interactive: bool = False,
    resume: bool = False,
) -> dict:
    """
    Run the Author mode pipeline end-to-end.

    Args:
        topic: Article topic
        keywords: List of SEO keywords (first is primary)
        thread_id: Existing thread ID for resume; generates new UUID if None
        interactive: If True, prompts user at each checkpoint; if False, auto-approves
        resume: If True, resumes from existing thread_id state

    Returns:
        Final graph state dict
    """
    if keywords is None:
        keywords = ["content marketing", "blog"]

    if thread_id is None:
        thread_id = str(uuid.uuid4())

    config = {"configurable": {"thread_id": thread_id}}
    graph = compile_graph()

    print(f"\nAuthor Mode Pipeline")
    print(f"Topic: {topic}")
    print(f"Keywords: {keywords}")
    print(f"Thread ID: {thread_id}")
    print(f"Mode: {'Interactive' if interactive else 'Auto-approve'}")
    print(f"{'Resume' if resume else 'Fresh run'}")

    if resume:
        # Resume from last checkpoint — pass Command(resume=None) to trigger re-evaluation
        print("\nResuming from last checkpoint...")
        result = graph.invoke(Command(resume={"approved": True}), config=config)
    else:
        # Fresh run
        initial_state = {
            "topic": topic,
            "keywords": keywords,
            "thread_id": thread_id,
            "search_cache": {},
            "cp1_iterations": 0,
            "cp2_iterations": 0,
            "metadata_saved": False,
            "duplicate_match": None,
            "duplicate_override": None,
            "research_report": None,
            "heading_structure": None,
            "cp1_approved": None,
            "cp1_feedback": None,
            "draft": None,
            "draft_validated": None,
            "cp2_approved": None,
            "cp2_feedback": None,
        }
        result = graph.invoke(initial_state, config=config)

    # Handle interrupt chain — loop until graph finishes or exits
    max_interrupts = 20  # safety limit
    interrupt_count = 0
    while result.get("__interrupt__") and interrupt_count < max_interrupts:
        resume_value = _handle_interrupt(result, interactive)
        result = graph.invoke(Command(resume=resume_value), config=config)
        interrupt_count += 1

    # Final state
    print(f"\n{'='*60}")
    print("PIPELINE COMPLETE")
    print(f"{'='*60}")
    print(f"Metadata saved: {result.get('metadata_saved', False)}")
    if result.get("draft"):
        word_count = len(result["draft"].split())
        print(f"Draft word count: {word_count}")
        print(f"Draft validated: {result.get('draft_validated')}")
        print(f"\nDraft preview (first 300 chars):\n{result['draft'][:300]}...")

    return result


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Bond Author Mode test harness")
    parser.add_argument("--topic", default="Jak zwiększyć ruch na blogu firmowym")
    parser.add_argument("--keywords", default="SEO blog,content marketing,ruch organiczny")
    parser.add_argument("--thread-id", default=None)
    parser.add_argument("--interactive", action="store_true")
    parser.add_argument("--resume", action="store_true")
    args = parser.parse_args()

    keywords = [k.strip() for k in args.keywords.split(",")]
    run_author_pipeline(
        topic=args.topic,
        keywords=keywords,
        thread_id=args.thread_id,
        interactive=args.interactive,
        resume=args.resume,
    )
```
  </action>
  <verify>
```bash
cd /Users/franciszekmarzynski/Downloads/Bond-agent

# Verify harness imports correctly
python -c "
from bond.harness import run_author_pipeline
print('harness importable:', callable(run_author_pipeline))
"

# Verify harness --help works
python -m bond.harness --help
```
Both commands complete without ImportError. Help text is displayed.
  </verify>
  <done>bond/harness.py imports without error. python -m bond.harness --help displays usage. run_author_pipeline is importable as a function.</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <name>Task 3: End-to-end Author mode pipeline verification</name>
  <action>Human verifies that the end-to-end Author mode pipeline produced a valid draft and saved metadata.</action>
  <what-built>
Complete Author mode Python backend: all 7 LangGraph nodes implemented (duplicate_check, researcher, structure, checkpoint_1, writer, checkpoint_2, save_metadata), SqliteSaver persistence, Metadata Log SQLite + ChromaDB, and CLI test harness.
  </what-built>
  <how-to-verify>
Run the test harness in auto-approve mode with a real topic. Requires EXA_API_KEY and at least one LLM key (ANTHROPIC_API_KEY or OPENAI_API_KEY) in .env:

```bash
cd /Users/franciszekmarzynski/Downloads/Bond-agent

# Ensure .env exists with API keys (copy from .env.example and fill in)
cp .env.example .env  # if not done already
# Edit .env: set EXA_API_KEY, ANTHROPIC_API_KEY or OPENAI_API_KEY, RESEARCH_MODEL, DRAFT_MODEL

# Run full pipeline in auto-approve mode
python -m bond.harness --topic "Jak pisać artykuły blogowe które pozycjonują się w Google" --keywords "pisanie artykułów SEO,pozycjonowanie bloga,content marketing"
```

Expected output:
1. Pipeline starts and runs duplicate check (no match on first run)
2. Exa research runs and produces a report
3. Structure node generates H1/H2/H3 outline
4. "[AUTO] Zatwierdzam strukturę nagłówków." appears (Checkpoint 1 auto-approved)
5. Writer node generates a draft (may take 30-60 seconds)
6. "[AUTO] Zatwierdzam draft." appears (Checkpoint 2 auto-approved)
7. "PIPELINE COMPLETE" with "Metadata saved: True" and draft word count >= 800

After the run, verify persistence:
```bash
python -c "
from bond.db.metadata_log import get_recent_articles
from bond.store.chroma import get_or_create_metadata_collection
rows = get_recent_articles(limit=3)
print('Metadata log rows:', len(rows))
for r in rows:
    print(f'  - topic: {r[\"topic\"][:50]}, date: {r[\"published_date\"]}')
col = get_or_create_metadata_collection()
print('ChromaDB metadata embeddings:', col.count())
"
```
Expected: >= 1 row in metadata_log, >= 1 embedding in ChromaDB.
  </how-to-verify>
  <resume-signal>Type "approved" if the pipeline ran to completion with Metadata saved: True and a draft was produced. Describe any errors or unexpected behavior if the run failed.</resume-signal>
</task>

</tasks>

<verification>
After all tasks and checkpoint complete:
```bash
cd /Users/franciszekmarzynski/Downloads/Bond-agent

# All 7 nodes are real (no stubs)
python -c "
from bond.graph.graph import compile_graph
graph = compile_graph()
expected = {'duplicate_check', 'researcher', 'structure', 'checkpoint_1', 'writer', 'checkpoint_2', 'save_metadata'}
actual = set(graph.nodes.keys())
missing = expected - actual
assert not missing, f'Missing nodes: {missing}'
print('All 7 nodes registered:', sorted(actual))
"

# Metadata persistence from harness run
python -c "
from bond.db.metadata_log import get_recent_articles
rows = get_recent_articles()
assert len(rows) >= 1, 'No metadata rows found'
print(f'Metadata log: {len(rows)} article(s) saved')
"

# ChromaDB embedding for future duplicate detection
python -c "
from bond.store.chroma import get_or_create_metadata_collection
col = get_or_create_metadata_collection()
assert col.count() >= 1, 'No embeddings in metadata collection'
print(f'ChromaDB metadata embeddings: {col.count()}')
"
```
</verification>

<success_criteria>
- checkpoint_2_node: single interrupt() call; soft cap warning appears at >= 3 iterations; targeted feedback captured in cp2_feedback
- save_metadata_node: writes SQLite row to bond_metadata.db AND adds ChromaDB embedding to bond_metadata_log_v1
- bond/harness.py: `python -m bond.harness --help` works; auto-approve mode drives pipeline to completion
- End-to-end pipeline run (with real API keys): produces draft with >= 800 words, saves metadata to both SQLite and ChromaDB, exits with "Metadata saved: True"
- A second harness run on the same topic triggers the duplicate check interrupt (similarity >= 0.85)
</success_criteria>

<output>
After completion, create `.planning/phases/02-author-mode-backend/02-04-SUMMARY.md` documenting:
- Checkpoint 2 soft cap behavior
- Metadata save dual-write (SQLite + ChromaDB) and the DUPL-01 loop it closes
- Test harness design decisions (auto-approve, interactive, resume modes)
- Full pipeline run results (word count, validation status)
- Any deviations from planned approach
</output>
